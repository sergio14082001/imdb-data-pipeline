# ğŸ¬ IMDb Data Pipeline Project 

Este proyecto de IngenierÃ­a de Datos tiene como objetivo construir un pipeline de datos que descarga, transforma y almacena informaciÃ³n proveniente del IMDb Datasets, utilizando herramientas comÃºnmente utilizadas en el stack de un Data Engineer.

ğŸ“Œ Objetivos del proyecto
Descargar datos reales del portal IMDb (archivos .tsv.gz).

- Limpiar y transformar los datos utilizando PySpark.

- Almacenar los resultados en formato Parquet y Delta Lake.

- Automatizar el pipeline con Apache Airflow (fase final).

- Construir un portafolio tÃ©cnico sÃ³lido con herramientas reales.

ğŸ§° TecnologÃ­as utilizadas
Herramienta |	PropÃ³sito
|-|-|
Python |	ProgramaciÃ³n principal
PySpark |	Procesamiento distribuido
Docker | Contenedores
Apache Airflow |	OrquestaciÃ³n de pipelines
Git & GitHub |	Control de versiones

---

ğŸš€ Descarga de Datos <br/>

Este proceso se encarga de:

1. Conectarse al portal oficial de IMDb Datasets.

2. Descargar los archivos comprimidos:

   - title.basics.tsv.gz
   
   - title.ratings.tsv.gz

3. Guardarlos en la carpeta local dags/data/.

---

ğŸ“Š Procesamiento y transformaciÃ³n de datos

Luego de la descarga, los archivos `.tsv.gz` fueron procesados con PySpark. A continuaciÃ³n se detallan los pasos realizados:

1. **Lectura de archivos brutos**:  
   - `title.basics.tsv.gz`: InformaciÃ³n bÃ¡sica de tÃ­tulos.  
   - `title.ratings.tsv.gz`: Calificaciones y cantidad de votos.

2. **TransformaciÃ³n de datos**:  
   - Se aplicaron filtros para seleccionar Ãºnicamente pelÃ­culas (`titleType == 'movie'`).
   - Se eliminaron valores nulos en campos crÃ­ticos como `primaryTitle` y `startYear`.
   - Se unieron los datasets mediante el campo `tconst`.

3. **ExportaciÃ³n en formatos eficientes**:  
   - Los resultados fueron almacenados en formato **Parquet**, optimizando lectura y almacenamiento.

---

ğŸ“ˆ VisualizaciÃ³n con Power BI

Se diseÃ±Ã³ un **dashboard interactivo en Power BI** a partir de los datos transformados, cargando los archivos Parquet del conjunto final (`TOP_MOVIES`).  
Este dashboard permite:

- Visualizar pelÃ­culas con mayor rating y mayor nÃºmero de votos.
- Filtrar por aÃ±os de estreno.
- Explorar tendencias en calificaciÃ³n y producciÃ³n cinematogrÃ¡fica.

> âœ… Todos los archivos `.parquet` fueron importados exitosamente desde una carpeta, usando la opciÃ³n **"Carpeta"** en Power BI para combinar mÃºltiples archivos en una sola tabla.

---

ğŸ›« OrquestaciÃ³n del pipeline con Apache Airflow

En la etapa final del proyecto, se utilizÃ³ Apache Airflow para automatizar el flujo de trabajo completo, desde la descarga de datos hasta el procesamiento y almacenamiento final. Esta es una prÃ¡ctica comÃºn en entornos productivos donde se requiere que los pipelines se ejecuten de forma recurrente y controlada.

ğŸ“‚ Estructura de archivos relevante

```
Airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ imdb_pipeline_dag.py      # DAG principal del proyecto
â”‚   â”œâ”€â”€ data/                     # Archivos .tsv.gz y carpeta processed/
â”‚   â””â”€â”€ scripts/                  # Scripts Python utilizados en cada tarea
â”œâ”€â”€ docker-compose.yaml          # ConfiguraciÃ³n de servicios Airflow
â”œâ”€â”€ Dockerfile                   # InstalaciÃ³n de dependencias adicionales
â”œâ”€â”€ plugins/
â””â”€â”€ logs/
```

âš™ï¸ ConfiguraciÃ³n del entorno con Docker

Se utilizÃ³ Docker para levantar todos los servicios necesarios de Airflow:

- Webserver: Interfaz web de monitoreo.

- Scheduler: Encargado de programar y ejecutar tareas.

- Postgres: Base de datos backend para almacenar metadatos.

(No se utilizaron workers ni Celery, dado que se usÃ³ el SequentialExecutor por simplicidad.)


```
# Desde la carpeta Airflow/
docker-compose up -d
```

Una vez levantado, puedes acceder a la interfaz de Airflow en:

```
http://localhost:8080
```

Credenciales por defecto:

- Usuario: airflow

- ContraseÃ±a: airflow

---

ğŸ“Œ DAG: imdb_pipeline_dag

El DAG imdb_pipeline_dag.py define las siguientes tareas:

1. Descargar datos: Ejecuta download_data.py.

2. Procesar datos: Ejecuta process_data.py.

3. Unir datasets: Ejecuta joined_data.py.

4. Filtrar top pelÃ­culas: Ejecuta filter_top_movies.py.

Cada tarea se ejecuta de forma secuencial, y los resultados finales se almacenan en formato Parquet dentro de la carpeta data/processed/.

---

ğŸ“Œ PrÃ³ximos pasos y mejoras sugeridas

- ğŸŒ Publicar el dashboard final en Power BI Service o Tableau Public.

---

ğŸ™‹ Autor

**Sergio SÃ¡nchez**  
Ingeniero de Sistemas con enfoque en ingenierÃ­a de datos y visualizaciÃ³n.  
[LinkedIn](www.linkedin.com/in/sergio-sanchez-rojas-161728208) | [GitHub](https://github.com/sergio14082001)

---
